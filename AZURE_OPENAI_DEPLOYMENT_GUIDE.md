# Azure OpenAI Deployment Guide for SoundScribe

> **Complete guide for setting up high-performance Azure OpenAI deployments with Global Standard quotas**

---

## üéØ Overview

This guide covers upgrading from basic Azure OpenAI deployments to high-performance Global Standard deployments that eliminate rate limiting issues and enable instant processing for the SoundScribe platform.

## üìä Before vs After

### Before (Standard Deployment)
- ‚ùå 40,000 TPM rate limit
- ‚ùå Frequent 429 rate limit errors
- ‚ùå Processing delays and timeouts
- ‚ùå Poor user experience

### After (Global Standard Deployment)
- ‚úÖ 551,000 TPM rate limit (13.8x increase)
- ‚úÖ Zero rate limit errors
- ‚úÖ Instant audio processing
- ‚úÖ Excellent user experience

---

## üöÄ Quick Setup

### 1. Current Production Configuration

```bash
# Environment Variables (Already Configured)
AZURE_OPENAI_ENDPOINT=https://eastus.api.cognitive.microsoft.com/
AZURE_OPENAI_API_KEY=your-api-key
AZURE_OPENAI_API_VERSION=2024-10-01-preview

# High-Performance Deployments
AZURE_OPENAI_GPT4O_MINI_DEPLOYMENT=gpt-4o-mini    # 551,000 TPM Global Standard
AZURE_OPENAI_WHISPER_DEPLOYMENT=whisper-1         # High-quota Whisper deployment
```

### 2. Deployment Status ‚úÖ

Both deployments are already configured and operational:

| Model | Deployment Name | Type | TPM Limit | Status |
|-------|----------------|------|-----------|---------|
| gpt-4o-mini | gpt-4o-mini | Global Standard | 551,000 | ‚úÖ Active |
| whisper-1 | whisper-1 | Standard | High-quota | ‚úÖ Active |

---

## üìã Step-by-Step Setup Guide

### Step 1: Access Azure Portal
1. Navigate to [Azure Portal](https://portal.azure.com)
2. Go to your Azure OpenAI resource
3. Select **Quotas** from the left sidebar

### Step 2: Request Quota Increase
1. **Select Model**: Choose `gpt-4o-mini`
2. **Current Quota**: Note existing TPM limit
3. **Request New Quota**: Enter `551000` (551,000 TPM)
4. **Deployment Type**: Select **"Global Standard"**
5. **Business Justification**: 
   ```
   Production AI-powered sales call analysis platform requiring 
   real-time audio transcription and analysis. High TPM needed 
   for concurrent user processing without delays.
   ```

### Step 3: Create Deployment
1. Go to **Deployments** section
2. Click **Create new deployment**
3. **Model**: Select `gpt-4o-mini`
4. **Deployment name**: `gpt-4o-mini`
5. **Deployment type**: `Global Standard`
6. **Tokens per minute rate limit**: `551000`
7. **Enable dynamic quota**: Yes (recommended)

### Step 4: Update Environment Variables
```bash
# Supabase Edge Functions
AZURE_OPENAI_GPT4O_MINI_DEPLOYMENT=gpt-4o-mini

# Azure App Service
AZURE_OPENAI_GPT4O_MINI_DEPLOYMENT=gpt-4o-mini
```

### Step 5: Verify Configuration
```bash
# Test the deployment
curl -X POST "https://eastus.api.cognitive.microsoft.com/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-10-01-preview" \
  -H "Content-Type: application/json" \
  -H "api-key: YOUR_API_KEY" \
  -d '{
    "messages": [{"role": "user", "content": "Test message"}],
    "max_tokens": 10
  }'
```

---

## üîß Technical Implementation

### Automatic Retry Logic

The SoundScribe platform includes built-in rate limit handling:

```typescript
// /echo-ai-scribe-app/supabase/functions/_shared/azure-openai.ts
async createTranscription(request: TranscriptionRequest): Promise<any> {
  const maxRetries = 3;
  let retryCount = 0;
  
  while (retryCount <= maxRetries) {
    try {
      const response = await fetch(url, { /* ... */ });
      
      if (response.ok) {
        return await response.json();
      }
      
      // Handle 429 rate limiting
      if (response.status === 429) {
        const waitTime = Math.pow(2, retryCount) * 1000; // Exponential backoff
        await new Promise(resolve => setTimeout(resolve, waitTime));
        retryCount++;
        continue;
      }
      
    } catch (error) {
      // Error handling...
    }
  }
}
```

### Deployment Types Comparison

| Feature | Standard | Global Standard |
|---------|----------|-----------------|
| **Base TPM** | 40,000 | 551,000+ |
| **Latency** | Regional | Global optimized |
| **Availability** | Regional | Multi-region |
| **Cost** | Lower | Higher |
| **Use Case** | Development | Production |

---

## üìà Performance Monitoring

### Rate Limit Tracking
```bash
# Monitor current usage
curl -H "api-key: YOUR_API_KEY" \
  "https://eastus.api.cognitive.microsoft.com/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-10-01-preview"

# Check response headers:
# x-ratelimit-remaining-tokens: 550,950
# x-ratelimit-remaining-requests: 9,999
```

### Key Metrics to Monitor
- **TPM Usage**: Tokens per minute consumption
- **Request Success Rate**: 200 vs 429 responses  
- **Response Latency**: Average processing time
- **Error Patterns**: Common failure modes

---

## ‚ö†Ô∏è Troubleshooting

### Common Issues

#### 1. 429 Rate Limit Errors
**Symptoms**: `Exceeded token rate limit` errors
**Solution**: 
- Verify Global Standard deployment is active
- Check current TPM usage in Azure Portal
- Request additional quota if needed

#### 2. Deployment Not Found
**Symptoms**: `DeploymentNotFound` errors
**Solution**:
- Verify deployment name matches environment variable exactly
- Check deployment status in Azure Portal
- Ensure deployment is successfully created

#### 3. Authentication Errors
**Symptoms**: `401 Unauthorized` responses
**Solution**:
- Verify API key is correct and active
- Check endpoint URL matches your resource region
- Ensure API version is supported

### Error Recovery
```typescript
// Example error handling pattern
try {
  const result = await azureOpenAIClient.createChatCompletion(request);
  return result;
} catch (error) {
  if (error.message.includes('429')) {
    // Rate limit - retry with backoff
    await waitAndRetry();
  } else if (error.message.includes('401')) {
    // Auth error - check credentials
    throw new Error('Authentication failed. Check API key.');
  } else {
    // Other errors
    console.error('Azure OpenAI Error:', error);
    throw error;
  }
}
```

---

## üí∞ Cost Optimization

### Token Usage Best Practices
1. **Optimize Prompts**: Use concise, effective prompts
2. **Batch Requests**: Group multiple operations when possible
3. **Cache Results**: Store frequently accessed responses
4. **Monitor Usage**: Track token consumption patterns

### Cost Estimation
```
Global Standard Pricing (Example):
- Input tokens: $0.000150 per 1K tokens
- Output tokens: $0.000600 per 1K tokens

Typical SoundScribe usage:
- 30-minute call: ~15,000 tokens
- Cost per call: ~$2.25 (including analysis)
```

---

## üîÑ Migration Checklist

### Pre-Migration
- [ ] Document current deployment settings
- [ ] Export usage metrics and performance data
- [ ] Backup existing configuration
- [ ] Test quota increase request process

### Migration Steps
- [ ] Request quota increase to 551,000 TPM
- [ ] Create Global Standard deployment
- [ ] Update environment variables
- [ ] Test new deployment thoroughly
- [ ] Monitor performance metrics

### Post-Migration
- [ ] Verify rate limits are resolved
- [ ] Monitor cost implications
- [ ] Update documentation
- [ ] Train team on new capabilities

---

## üìû Support & Resources

### Getting Help
- **Azure Support**: Create support ticket for quota increases
- **Documentation**: [Azure OpenAI Service docs](https://docs.microsoft.com/en-us/azure/cognitive-services/openai/)
- **Pricing**: [Azure OpenAI pricing details](https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/)

### Useful Azure CLI Commands
```bash
# List current deployments
az cognitiveservices account deployment list \
  --name "your-openai-resource" \
  --resource-group "your-resource-group"

# Create new deployment
az cognitiveservices account deployment create \
  --name "your-openai-resource" \
  --resource-group "your-resource-group" \
  --deployment-name "gpt-4o-mini" \
  --model-name "gpt-4o-mini" \
  --model-version "2024-07-18" \
  --sku-capacity 551 \
  --sku-name "GlobalStandard"
```

---

## ‚úÖ Success Validation

### Performance Benchmarks
After implementing Global Standard deployment, you should see:

| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| TPM Limit | 40,000 | 551,000 | +1,277% |
| 429 Errors | Common | Eliminated | 100% reduction |
| Processing Time | 30-60s | 3-5s | 85% faster |
| User Satisfaction | Low | High | Significantly improved |

### Final Verification
```bash
# Test high-volume processing
curl -X POST "https://eastus.api.cognitive.microsoft.com/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-10-01-preview" \
  -H "Content-Type: application/json" \
  -H "api-key: YOUR_API_KEY" \
  -d '{
    "messages": [{"role": "user", "content": "Process this sales call..."}],
    "max_tokens": 1000
  }'

# Should return success with no rate limiting
```

---

**üéâ Congratulations!** You now have a high-performance Azure OpenAI setup capable of handling enterprise-scale audio processing without rate limiting issues.

---

*Last Updated: June 26, 2025*  
*SoundScribe Platform - Azure OpenAI Global Standard Configuration*